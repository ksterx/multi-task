import torch
import torch.optim as optim
import wandb
from datasets import load_dataset
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm, trange


class TextWithTokenSentimentDataset(Dataset):
    def __init__(self, texts, word_sentiments, tokenizer, max_length=32):
        """
        texts: list of strings (å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ)
        word_sentiments: list of lists (å˜èªã”ã¨ã®æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«)
            e.g. word_sentiments[i] = [0,1,2] -> iç•ªç›®ã®ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾å¿œã™ã‚‹å˜èªã”ã¨ã®æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«
        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶
        max_length: æœ€å¤§é•·ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚„åˆ‡ã‚Šè©°ã‚å‡¦ç†ã«ä½¿ç”¨ï¼‰
        """
        self.texts = texts
        self.word_sentiments = word_sentiments  # å„å˜èªã”ã¨ã®æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        word_labels = self.word_sentiments[
            idx
        ]  # ä¾‹: [0, 0, 1] ã®ã‚ˆã†ãªå˜èªå˜ä½ã®æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«

        # 1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º (subwordå˜ä½ã«åˆ†å‰²)
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt",
            return_offsets_mapping=True,  # å˜èªå˜ä½ã§ã®ä½ç½®æƒ…å ±ã‚’å–å¾—
        )
        input_ids = encoding["input_ids"].squeeze(0)  # (seq_len,)
        attention_mask = encoding["attention_mask"].squeeze(0)  # (seq_len,)
        offset_mapping = encoding["offset_mapping"].squeeze(0)  # (seq_len, 2)

        seq_len = input_ids.size(0)  # ãƒˆãƒ¼ã‚¯ãƒ³æ•°

        # 2. æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ã¸ã®æ‹¡å¼µ
        token_sentiments = torch.ones(seq_len, dtype=torch.long) * (
            -100
        )  # åˆæœŸå€¤ã¯ignore_index (-100)
        word_index = 0  # å˜èªãƒ©ãƒ™ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

        for token_idx, (start, end) in enumerate(offset_mapping):
            if start == 0 and end != 0:  # æ–°ã—ã„å˜èªã®é–‹å§‹ï¼ˆé€šå¸¸ã€æœ€åˆã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ï¼‰
                if word_index < len(word_labels):
                    token_sentiments[token_idx] = word_labels[
                        word_index
                    ]  # å˜èªã®æ„Ÿæƒ…ã‚’é©ç”¨
                    word_index += 1  # æ¬¡ã®å˜èªã¸
            else:
                # ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³ã¯å‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ©ãƒ™ãƒ«ã‚’ã‚³ãƒ”ãƒ¼
                if token_idx > 0:
                    token_sentiments[token_idx] = token_sentiments[token_idx - 1]

        # 3. LM ç”¨ã®ãƒ©ãƒ™ãƒ«
        labels = input_ids.clone()

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels,  # è¨€èªãƒ¢ãƒ‡ãƒ«ç”¨ (seq_len,)
            "branch_labels": token_sentiments,  # æ„Ÿæƒ…åˆ†é¡ç”¨ (seq_len,)
        }


def get_dataloader(tokenizer, batch_size=2, max_length=2048):
    train_dataset = load_dataset("mteb/tweet_sentiment_extraction", split="train")
    test_dataset = load_dataset("mteb/tweet_sentiment_extraction", split="test")

    train_texts = train_dataset["text"]
    train_token_sentiments = [[s] for s in train_dataset["label"]]

    test_texts = test_dataset["text"]
    test_token_sentiments = [[s] for s in test_dataset["label"]]

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ & ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€
    train_dataset = TextWithTokenSentimentDataset(
        texts=train_texts,
        word_sentiments=train_token_sentiments,
        tokenizer=tokenizer,
        max_length=max_length,
    )
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    test_dataset = TextWithTokenSentimentDataset(
        texts=test_texts,
        word_sentiments=test_token_sentiments,
        tokenizer=tokenizer,
        max_length=max_length,
    )
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, test_loader


def train(
    model,
    train_loader,
    eval_loader,
    save_path="./",
    log_steps=100,
    eval_and_save_steps=500,
    num_epochs=10,
):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
    best_eval_loss = float("inf")

    for epoch in range(num_epochs):
        model.train()

    optimizer = optim.Adam(model.parameters(), lr=1e-4)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    optimizer = optim.Adam(model.parameters(), lr=1e-4)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    global_step = 0

    num_epochs = 5
    for _ in trange(num_epochs):
        model.train()
        total_loss = 0.0
        total_lm_loss = 0.0
        total_branch_loss = 0.0

        for batch in tqdm(train_loader):
            # ãƒãƒƒãƒã‚’GPUã¸
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)  # LMç”¨
            branch_labels = batch["branch_labels"].to(device)  # æ„Ÿæƒ…åˆ†é¡ç”¨

            # å‹¾é…ã‚¯ãƒªã‚¢
            optimizer.zero_grad()

            # Forward
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
                branch_labels=branch_labels,
                return_dict=True,
            )
            loss = outputs.loss

            loss_lm = (
                outputs.loss_backbone
                if hasattr(outputs, "loss_backbone")
                else torch.tensor(0.0)
            )
            loss_branch = (
                outputs.loss_branch
                if hasattr(outputs, "loss_branch")
                else torch.tensor(0.0)
            )

            # Backward & update
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            total_lm_loss += loss_lm.item()
            total_branch_loss += loss_branch.item()

            global_step += 1

            # ğŸ“ log_steps ã”ã¨ã« wandb ã«è¨˜éŒ²
            if global_step % log_steps == 0:
                wandb.log(
                    {
                        "Train/Step Loss": loss.item(),
                        "Train/Step LM Loss": loss_lm.item(),
                        "Train/Step Branch Loss": loss_branch.item(),
                    }
                )

            # ğŸ“Œ eval_and_save_steps ã”ã¨ã«è©•ä¾¡ã¨ä¿å­˜
            if global_step % eval_and_save_steps == 0 and global_step > 0:
                avg_eval_loss = evaluate(
                    model, eval_loader, device, save_path, global_step
                )
                print(f"ğŸ“Š Eval Step {global_step}, Loss: {avg_eval_loss:.4f}")

                # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ï¼ˆEval Loss ãŒæœ€å°ã®å ´åˆã®ã¿ä¿å­˜ï¼‰
                if avg_eval_loss < best_eval_loss:
                    best_eval_loss = avg_eval_loss
                    torch.save(model.state_dict(), save_path)
                    print(
                        f"âœ… Model saved at step {global_step} with Eval Loss {best_eval_loss:.4f}"
                    )


def evaluate(model, eval_loader, device, save_path, global_step):
    model.eval()
    total_eval_loss = 0.0
    total_eval_lm_loss = 0.0
    total_eval_branch_loss = 0.0

    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in tqdm(eval_loader):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)
            branch_labels = batch["branch_labels"].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
                branch_labels=branch_labels,
                return_dict=True,
            )
            loss = outputs.loss
            loss_lm = (
                outputs.loss_backbone
                if hasattr(outputs, "loss_backbone")
                else torch.tensor(0.0)
            )
            loss_branch = (
                outputs.loss_branch
                if hasattr(outputs, "loss_branch")
                else torch.tensor(0.0)
            )

            total_eval_loss += loss.item()
            total_eval_lm_loss += loss_lm.item()
            total_eval_branch_loss += loss_branch.item()

            # æ„Ÿæƒ…åˆ†é¡ã®äºˆæ¸¬
            branch_logits = outputs.branch_logits  # (batch, seq_len, 3)
            preds = torch.argmax(branch_logits, dim=-1)  # (batch, seq_len)
            mask = branch_labels != -100  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°éƒ¨åˆ†ã‚’ç„¡è¦–
            preds = preds[mask]
            labels = branch_labels[mask]

            all_preds.extend(preds.cpu().tolist())
            all_labels.extend(labels.cpu().tolist())

    avg_eval_loss = total_eval_loss / len(eval_loader)
    avg_eval_lm_loss = total_eval_lm_loss / len(eval_loader)
    avg_eval_branch_loss = total_eval_branch_loss / len(eval_loader)

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
    accuracy = accuracy_score(all_labels, all_preds)
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_labels, all_preds, average="macro", zero_division=0
    )

    print(
        f"ğŸ“Š Eval Results - Loss: {avg_eval_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}"
    )

    wandb.log(
        {
            "Eval/Loss": avg_eval_loss,
            "Eval/LM Loss": avg_eval_lm_loss,
            "Eval/Branch Loss": avg_eval_branch_loss,
            "Eval/Accuracy": accuracy,
            "Eval/Precision": precision,
            "Eval/Recall": recall,
            "Eval/F1-score": f1,
        }
    )

    model.save_pretrained(save_path + f"step_{global_step}")

    return avg_eval_loss
